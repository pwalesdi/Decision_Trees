{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to build Decision Trees\n",
    "Authors: Patrick Wales-Dinan\n",
    "\n",
    "<img src=\"./images/new_job.png\" align=\"left\" width=1000>\n",
    "\n",
    "- (This image is courtesy of [Rajesh Brid](https://medium.com/@rajesh_brid).)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "- Understand what a decision tree is.\n",
    "- Calculate Gini Impurity.\n",
    "- Describe how decision trees use Gini Impurity to make decisions.\n",
    "- Fit, generate predictions from, and evaluate decision tree models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What should we do today?\n",
    "\n",
    "|$Y = $ Activity|$X_1 = $ Day|$X_2 = $ Weather|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|   Movies  |      Weekend     |   Rainy  |\n",
    "|   Netflix |      Weekday     |   Sunny  |\n",
    "|   Beach   |      Weekend     |   Sunny  |\n",
    "|   Netflix |      Weekday     |   Rainy  |\n",
    "|   Netflix |      Weekday     |   Rainy  |\n",
    "|   Beach   |      Weekend     |   Sunny  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>It's a weekday. Based on our past behavior, what do you think we'll do today?</summary>\n",
    "\n",
    "- Watch Netflix.\n",
    "- In 100% of past cases where it's a weekday we've watched Netflix!\n",
    "\n",
    "|$Y = $ Activity|$X_1 = $ Day|$X_2 = $ Weather|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|   Netflix  |      Weekday     |   Sunny  |\n",
    "|   Netflix  |      Weekday     |   Rainy  |\n",
    "|   Netflix  |      Weekday     |   Rainy  |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>It's a weekend. Based on our past behavior, what do you think we'll do?</summary>\n",
    "\n",
    "- Either go to the movies or got to the beach... but we can't say with certainty whether we'd go to the beach or go to the movies.\n",
    "- Based on our past behavior, we go to the movies on 1/3 of weekend days and we go to the beach on 2/3 of weekend days. (You can think of `.predictproba()`)\n",
    "- If I **had** to make a guess here, I'd probably predict that we would go to the beach, but we may want to use additional information to be certain.\n",
    "\n",
    "|$Y = $ Activity|$X_1 = $ Day|$X_2 = $ Weather|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|  Movies |      Weekend     |   Rainy  |\n",
    "|  Beach  |      Weekend     |   Sunny  |\n",
    "|  Beach  |      Weekend     |   Sunny  |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>It's the weekend and the weather is sunny! Based on our past behavior, what do you think we'll do?</summary>\n",
    "\n",
    "- Go to the beach.\n",
    "- In 100% of past cases where the weather is sunny and where it's a weekend, we've gone to the beach.\n",
    "\n",
    "|$Y = $ Activity|$X_1 = $ Day|$X_2 = $ Weather|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|  Beach  |      Weekend     |   Sunny  |\n",
    "|  Beach  |      Weekend     |   Sunny  |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees: Overview\n",
    "\n",
    "A decision tree:\n",
    "- takes a dataset consisting of $X$ and $Y$ data, \n",
    "- finds rules based on our $X$ data that partitions (splits) our data into smaller datasets such that\n",
    "- by the bottom of the tree, the values $Y$ in each \"leaf node\" are as \"pure\" as possible.\n",
    "\n",
    "We frequently see decision trees represented by a graph.\n",
    "\n",
    "<img src=\"./images/decision_tree_1.png\" alt=\"what_to_do\" width=\"750\"/>\n",
    "\n",
    "- (This image was created using [Draw.io](https://www.draw.io/).)\n",
    "\n",
    "### Terminology\n",
    "Decision trees look like upside down trees. \n",
    "- What we see on top is known as the \"root node,\" through which all of our observations are passed.\n",
    "- At each internal split, our dataset is partitioned.\n",
    "- A \"parent\" node is split into two or more \"child\" nodes.\n",
    "- At each of the \"leaf nodes\" (colored orange), we contain a subset of records that are as pure as possible.\n",
    "    - In the example above, each leaf node is perfectly pure. Once we get to a leaf node, every observation in that leaf node has the exact same value of $Y$!\n",
    "    - There are ways to quantify the idea of \"purity\" here so that we can let our computer do most of the tree-building (model-fitting) process... we'll come back to this later.\n",
    "\n",
    "Decision trees are also called \"**Classification and Regression Trees**,\" sometimes abbreviated \"**CART**.\"\n",
    "- [DecisionTreeClassifier Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "- [DecisionTreeRegressor Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 Questions\n",
    "\n",
    "If you aren't familiar with the game [20 Questions](https://en.wikipedia.org/wiki/Twenty_Questions), it's a game with two players (or teams). \n",
    "- Player 1 thinks of an item.\n",
    "- Player 2 then tries to guess what the item is by asking a series of 20 questions that have a yes or no answer.\n",
    "- If player 2 correctly guesses the item within those 20 questions, then player 2 wins!\n",
    "- If player 2 does not correctly guess the item within the 20 question limit, then player 1 wins!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision trees operate in a fashion that's pretty similar to 20 Questions.\n",
    "- Decisions are made in a sequential fashion. Once you know a piece of information, you use that piece of information when asking future questions.\n",
    "    - Example: If you know that the item you're trying to guess is a person, then you can use that information to ask better subsequent questions.\n",
    "- It's possible to get lucky by making very specific guesses early, but it's pretty unlikely that this is a winning strategy.\n",
    "    - Example: If you asked, \"Is it an airplane? Is it a boat? Is it a car?\" as your first three questions, it's not very likely that you'll win the game.\n",
    "\n",
    "When fitting a decision tree, we're effectively getting our computer to play a game of 20 Questions. We give the computer some data and it figures out the best $X$ variable to split on at the right time.\n",
    "- Above, our \"what should we do today?\" decision tree first asked what type of day it was a weekend or a weekday, **then** asked what the weather was like.\n",
    "- If we had asked \"what the weather out\" first, we'd have ended up with a slightly more complicated decision tree.\n",
    "\n",
    "Just like with all of our models, in order for the computer to learn which $X$ variable to split on and when, the computer needs a loss function to quantify how good a particular split is. This is where the idea of **purity** comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purity in Decision Trees\n",
    "\n",
    "When quantifying how \"pure\" a node is, we want to see what the distribution of $Y$ is in each node, then summarize this distribution with a number.\n",
    "\n",
    "<img src=\"./images/decision_tree_1.png\" alt=\"what_to_do\" width=\"750\"/>\n",
    "\n",
    "- For continuous $Y$ (i.e. using a decision tree to predict income), the default option is mean squared error.\n",
    "- This is the `criterion = 'mse'` argument in `DecisionTreeRegressor`.    \n",
    "\n",
    "- For discrete $Y$, the default option is the Gini impurity. *(Bonus: This is not quite the same thing as the [Gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient).)*\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "\\text{Gini impurity (2 classes)} &=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 \\\\\n",
    "\\text{Gini impurity (3 classes)} &=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 - P(\\text{class 3})^2 \\\\\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our y variable from our \"what should we do\" dataframe.\n",
    "y = ['Movies', 'Netflix', 'Beach', 'Netflix', 'Netflix', 'Beach']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gini function, called gini.\n",
    "def gini(obs):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6111111111111112"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if your Gini function is correct on the \n",
    "# \"what should we do tonight?\" data. (Should get 0.6111.)\n",
    "gini(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Practice\n",
    "\n",
    "<details><summary>What is the Gini impurity of a node when every item is from the same class?</summary>\n",
    "    \n",
    "- Our Gini impurity is zero.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "&=& 1 - P(\\text{class 1})^2 \\\\\n",
    "&=& 1 - 1^2 \\\\\n",
    "&=& 1 - 1 \\\\\n",
    "&=& 0\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444444"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is Gini when every item is from the same class?\n",
    "gini(['Netflix', 'Netflix', 'Netflix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is the Gini impurity of a node when we have two classes, each with two items?</summary>\n",
    "    \n",
    "- Our Gini impurity is 0.5.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "&=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 \\\\\n",
    "&=& 1 - \\left(\\frac{1}{2}\\right)^2 - \\left(\\frac{1}{2}\\right)^2 \\\\\n",
    "&=& 1 - \\frac{1}{4} - \\frac{1}{4} \\\\\n",
    "&=& \\frac{1}{2}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is Gini when we have two classes, each with two items?\n",
    "gini(['Netflix', 'Netflix', 'Beach', 'Beach'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is the Gini impurity of a node when we have two classes, each with three items?</summary>\n",
    "    \n",
    "- Our Gini impurity is 0.5.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "&=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 \\\\\n",
    "&=& 1 - \\left(\\frac{1}{2}\\right)^2 - \\left(\\frac{1}{2}\\right)^2 \\\\\n",
    "&=& 1 - \\frac{1}{4} - \\frac{1}{4} \\\\\n",
    "&=& \\frac{1}{2}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is Gini when we have two classes, each with three items?\n",
    "gini(['Netflix', 'Netflix', 'Netflix', 'Beach', 'Beach', 'Beach'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is the Gini impurity of a node when we have three classes, each with two items?</summary>\n",
    "    \n",
    "- Our Gini impurity is 0.6667.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini impurity} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "&=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 - P(\\text{class 3})^2 \\\\\n",
    "&=& 1 - \\left(\\frac{1}{3}\\right)^2 - \\left(\\frac{1}{3}\\right)^2 - \\left(\\frac{1}{3}\\right)^2 \\\\\n",
    "&=& 1 - \\frac{1}{9} - \\frac{1}{9} - \\frac{1}{9} \\\\\n",
    "&=& 1 - \\frac{1}{3} \\\\\n",
    "&=& \\frac{2}{3}\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is Gini when we have three classes, each with two items?\n",
    "gini(['Netflix', 'Netflix', 'Beach', 'Beach', 'Movies', 'Movies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Summary of Gini Impurity Scores</summary>\n",
    "\n",
    "- In the binary case, Gini impurity ranges from 0 to 0.5.\n",
    "- If we have three classes, Gini impurity ranges from 0 to 0.66667.\n",
    "- If we have $k$ classes, Gini impurity ranges from 0 to $1-\\frac{1}{k}$.\n",
    "- In all cases, a Gini impurity of 0 means maximum purity - all of our observations are from the same class!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So how does a decision tree use Gini to decide which variable to split on?\n",
    "\n",
    "- At any node, consider the subset of our dataframe that exists at that node.\n",
    "- Iterate through each variable that could potentially split the data.\n",
    "- Calculate the Gini impurity for every possible split.\n",
    "- Select the variable that causes the greatest decrease in Gini impurity from the parent node to the child node.\n",
    "\n",
    "<details><summary>What is the decrease in Gini impurity if we split on $X_1$? (Weekend vs. Weekday)</summary>\n",
    "\n",
    "- Answer: 0.389\n",
    "\n",
    "<img src=\"./images/gini_decrease_1.png\" alt=\"gini_decrease\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is the decrease in Gini impurity if we split on $X_2$? (Sunny Day vs. Rainy Day)</summary>\n",
    "    \n",
    "- Answer: 0.167\n",
    "\n",
    "<img src=\"./images/gini_decrease_2.png\" alt=\"gini_decrease\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One consequence of this is that a decision tree is fit using a **greedy** algorithm. Simply put, a decision tree makes the best short-term decision by optimizing at each node individually. _This might mean that our tree isn't optimal in the long run!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data.\n",
    "import pandas as pd\n",
    "\n",
    "# Read in Titanic data.\n",
    "titanic = pd.read_csv('./titanic_clean.csv')\n",
    "\n",
    "# Change sex to float.\n",
    "titanic['Sex'] = titanic['Sex'].map({'male':0,\n",
    "                                     'female':1})\n",
    "\n",
    "# Create embarked_S column.\n",
    "titanic['Embarked_s'] = titanic['Embarked'].map({'S':1,\n",
    "                                                 'C':0,\n",
    "                                                 'Q':0})\n",
    "\n",
    "# Create embarked_C column.\n",
    "titanic['Embarked_c'] = titanic['Embarked'].map({'S':0,\n",
    "                                                 'C':1,\n",
    "                                                 'Q':0})\n",
    "\n",
    "# Conduct train/test split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(titanic.drop(['Survived','PassengerId','Name','Embarked'], axis=1),\n",
    "                                                    titanic['Survived'],\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out first five rows of X_train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What conclusion would you make here?</summary>\n",
    "\n",
    "- Our model is **very** overfit to the data.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fitting a decision tree, your model will always grow until it nearly perfectly predicts every observation!\n",
    "- This is like playing a game of 20 questions, but instead calling it \"Infinite Questions.\" You're always going to be able to win!\n",
    "\n",
    "<details><summary>Intuitively, what might you try to do to solve this problem?</summary>\n",
    "    \n",
    "- As with all models, try to gather more data.\n",
    "- As with all models, remove some features.\n",
    "- Is there a way for us to stop our model from growing? (Yes!)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of Decision Trees\n",
    "There are three hyperparameters of decision trees that we may commonly tune in order to prevent overfitting.\n",
    "\n",
    "- `max_depth`: The maximum depth of the tree.\n",
    "    - By default, the nodes are expanded until all leaves are pure (or some other argument limits the growth of the tree).\n",
    "    - In the 20 questions analogy, this is like \"How many questions we can ask?\"\n",
    "- `min_samples_split`: The minimum number of samples required to split an internal node.\n",
    "    - By default, the minimum number of samples required to split is 2. That is, if there are two observations in a node and if we haven't already achieved maximum purity, we can split it!\n",
    "- `min_samples_leaf`: The minimum number of samples required to be in a leaf node (a terminal node at the end of the tree).\n",
    "    - By default, the minimum nubmer of samples required in a leaf node is 1. (This should ring alarm bells - it's very possible that we'll overfit our model to the data!)\n",
    "\n",
    "[Source: Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model with:\n",
    "# - a maximum depth of 5.\n",
    "# - at least 7 samples required in order to split an internal node.\n",
    "# - at least 3 samples in each leaf node.\n",
    "# - random state of 42.\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=5, min_samples_split=7, min_samples_leaf=3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=3, min_samples_split=7,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model.\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training set: 0.8714859437751004\n",
      "Score on testing set: 0.7897196261682243\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model.\n",
    "print(f'Score on training set: {dt.score(X_train, y_train)}')\n",
    "print(f'Score on testing set: {dt.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I think this is where the lesson is over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's GridSearch to try to find the best tree.\n",
    "\n",
    "- Check [3, 5, 7, 10] for `max_depth`.\n",
    "- Check [5, 10, 15, 20] for `min_samples_split`.\n",
    "- Check [2, 3, 4, 5, 6, 7] for `min_samples_leaf`.\n",
    "- Run 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How many models are being fit here?</summary>\n",
    "\n",
    "- 4 * 4 * 6 * 5 = 480 models.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_params = {\n",
    "    'max_depth' : [3, 5, 7, 10],\n",
    "    'min_samples_split': [5, 10, 15, 20],\n",
    "    'min_samples_leaf': [2, 3, 4, 5, 6, 7]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42),\n",
    "                   param_grid=dt_params,\n",
    "                   cv=5,\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4272170066833496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 480 out of 480 | elapsed:    2.4s finished\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start our timer.\n",
    "t0 = time.time()\n",
    "\n",
    "# Let's GridSearch over the above parameters on our training data.\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Stop our timer and print the result.\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=6, min_samples_split=15,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is our best decision tree?\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 7, 'min_samples_leaf': 6, 'min_samples_split': 15}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What were the best parameters of our decision tree?\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8152610441767069"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What was the cross-validated score of the above decision tree?\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training set: 0.8775100401606426\n",
      "Score on testing set: 0.7523364485981309\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model with best parameters.\n",
    "dt = grid.best_estimator_ # this takes the best paramenters run from your grid search CV and runs it\n",
    "\n",
    "# Fit model.\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model.\n",
    "print(f'Score on training set: {dt.score(X_train, y_train)}')\n",
    "print(f'Score on testing set: {dt.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set.\n",
    "preds = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import confusion_matrix.\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101  21]\n",
      " [ 32  60]]\n"
     ]
    }
   ],
   "source": [
    "# Generate confusion matrix.\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,\n",
    "                                  preds).ravel()\n",
    "\n",
    "print(confusion_matrix(y_test,\n",
    "                       preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.6522\n"
     ]
    }
   ],
   "source": [
    "# Calculate sensitivity.\n",
    "\n",
    "sens = tp / (tp + fn)\n",
    "\n",
    "print(f'Sensitivity: {round(sens, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity: 0.8279\n"
     ]
    }
   ],
   "source": [
    "# Calculate specificity.\n",
    "\n",
    "spec = tn / (tn + fp)\n",
    "\n",
    "print(f'Specificity: {round(spec, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's so great about decision trees?\n",
    "\n",
    "\n",
    "### 1. We don't have to scale our data.\n",
    "Much in the same way that decision trees don't need much preprocessing in order to work, the base assumptions around how data is used and estimated, don't rely on scale to be effective.\n",
    "\n",
    "### 2. Decision trees don't care about how your data is distributed.\n",
    "\n",
    "Is your data heavily skewed or not normally distributed? Decision trees are nonparametric, meaning we don't make assumptions about how our data or errors are distributed.\n",
    "\n",
    "### 3. Easy to interpret.\n",
    "\n",
    "The output of a decision tree is easy to interpet and thus relatable to non-technical people. (We'll talk about `feature_importance` later.)\n",
    "\n",
    "### 4. Speed.\n",
    "\n",
    "Decision trees are fit very quickly!\n",
    "\n",
    "> **Protip**\n",
    ">\n",
    "> Consider creating a benchmark using a decision tree to understand how one might behave on its own before using a more complicated method.  Do your best to understand how a simple model behaves on a set of data, before using a more complex model. This is a great practice to get into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the downsides of decision trees?\n",
    "\n",
    "\n",
    "### 1. Decision trees can very easily overfit.\n",
    "Decision trees often suffer from high error due to variance, so we need to take special care to avoid this. (Luckily, there are lots of algorithms designed to do exactly this!)\n",
    "\n",
    "### 2. Decision trees are locally optimal.\n",
    "Because we're making the best decision at each node (greedy), we might end up with a worse solution in the long run.\n",
    "\n",
    "### 3. Decision trees don't work well with unbalanced data.\n",
    "We often will bias our results toward the majority class. We need to take steps to avoid this as well! (Check out the `class_weight` parameter if you're interested.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {
    "0b28c6b3b13649718658d43e965c8062": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
